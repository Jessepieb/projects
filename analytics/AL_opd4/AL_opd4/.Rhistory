cd C:\Users\Jesse\CLionProjects\untitled\cmake-build-debug
C:\Users\Jesse\CLionProjects\untitled\cmake-build-debug
test <- read.table("C:/Users/Jesse/CLionProjects/untitled/cmake-build-debug/test.dat", quote="\"", comment.char="")
View(test)
test <- read.table("C:/Users/Jesse/CLionProjects/untitled/cmake-build-debug/test.dat", quote="\"", comment.char="")
View(test)
logn = log10(test$V1)
mlogn= lm(test$V2 - logn)
mlogn= lm(test$V2 ~ logn)
summary(mlogn)
summary(mlogn)
nsq = test$V1^2
msnq = lm(test$V2 - nsq)
msnq = lm(test$V2 ~ nsq)
summary(msqn)
summary(msnq)
nlogn = test$V1 * log10(test$V1)
mnlogn = lm(test$V2 ~ nlogn)
summary(mnlogn)
plot(logn,test$V2)
abline(mlogn)
plot(nsq, test$V2)
ablie(mnsq)
ablie(msnq)
abline(msnq)
plot(nlogn, test$V2)
abline(mlogn)
abline(mnlogn)
rmline(mlogn)
plot(nlogn)
library(readr)
dataset <- read_csv(NULL)
View(dataset)
library(readr)
results <- read_csv("D:/GitHub/projects/algoritmiek/opd 1/SortTimer/test.csv")
ls()
rm(list = c("mlogn","mnlogn", "msnq","logn","nlogn","nsq","test"))
logn = log10(results$size)
rm(ls())
rm(ls
)
ls()
rm(*)
rm(list = ls())
library(readr)
results <- read_csv("D:/GitHub/projects/algoritmiek/opd 1/SortTimer/test.csv")
logn = log10(results$size)
mlogn = lm(results$duration - logn)
mlogn = lm(results$duration ~ logn)
summary(mlogn)
nsq = results$size^2
mnsq = lm(results$duration ~ nsq)
summary(mnsq)
nlogn = results$size * log10(results$size)
mnlogn = lm(results$duration ~ nlogn)
summary(mnlogn)
plot(logn, results$duration)
abline(mlogn)
plot(nsq, results$duration)
abline(mnsq)
plot(nlogn, results$duration)
abline(mnlogn)
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
install.packages(c("tm", "caTools", "SnowballC"))
install.packages(c("rpart", "rpart.plot", "ROCR"))
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
cv
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
library(tm)
library(SnowballC)
library(caret)
library(e1071)
#read data from csv file
twts = read.csv("twts.csv", stringsAsFactors = FALSE)
#extract only the needed data
twts = subset(
twts,
select = c(Sentiment, Tweet)
)
#add new factor Negative (TRUE or FALSE)
twts$Negative = as.factor(twts$Sentiment < 2)
#load tweets into the Corpus format (collection of documents)
# making it easy to apply modifications to the provided text
corpus = Corpus(VectorSource(twts$Tweet))
#set all characters to lower
corpus = tm_map(corpus, tolower)
#remove all punctuation
corpus = tm_map(corpus, removePunctuation)
#remove all stop-words, these are not needed in our model
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
#stem all words
corpus = tm_map(corpus, stemDocument)
#get frequencies of all the words in our corpus
frequencies = DocumentTermMatrix(corpus)
#remove rare words
sparse = removeSparseTerms(frequencies, 0.995)
#make sparse into a data frame
tweetSparse = as.data.frame(as.matrix(sparse))
#correcting names , words that start with numbers will be changed
colnames(tweetSparse) = make.names(colnames(tweetSparse))
#add independent variable
tweetSparse$Negative = twts$Negative
library(caTools)
#seed for pseudo random generator
set.seed(123)
#splitting the data
split = sample.split(tweetSparse$Negative, SplitRatio = 0.7)
#training set
trainsparse = subset(tweetSparse, split == TRUE)
#testing set
testsparse = subset(tweetSparse, split == FALSE)
library(rpart)
library(rpart.plot)
fitControl = trainControl(method = "cv", number = 10)
cartGrid = expand.grid(.cp=(1:50)*0.01)
cv = train(Negative~. , data = trainsparse, method="rpart",
trControl = fitControl, tuneGrid = cartGrid)
cv
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
#read data from csv file
twts = read.csv("twts.csv", stringsAsFactors = FALSE)
#extract only the needed data
twts = subset(
twts,
select = c(Sentiment, Tweet)
)
#load tweets into the Corpus format (collection of documents)
# making it easy to apply modifications to the provided text
corpus = Corpus(VectorSource(twts$Tweet))
#set all characters to lower
corpus = tm_map(corpus, tolower)
#remove all punctuation
corpus = tm_map(corpus, removePunctuation)
#remove all stop-words, these are not needed in our model
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
#stem all words
corpus = tm_map(corpus, stemDocument)
#get frequencies of all the words in our corpus
frequencies = DocumentTermMatrix(corpus)
#remove rare words
sparse = removeSparseTerms(frequencies, 0.995)
#make sparse into a data frame
tweetSparse = as.data.frame(as.matrix(sparse))
#correcting names , words that start with numbers will be changed
colnames(tweetSparse) = make.names(colnames(tweetSparse))
#add independent variable
tweetSparse$Sentiment = twts$Sentiment
table(tweetSparse$Sentiment)
library(caTools)
#seed for pseudo random generator
set.seed(123)
#splitting the data
split = sample.split(tweetSparse$Sentiment, SplitRatio = 0.7)
#training set
trainsparse = subset(tweetSparse, split == TRUE)
#testing set
testsparse = subset(tweetSparse, split == FALSE)
library(rpart)
library(rpart.plot)
fitControl = trainControl(method = "cv", number = 10)
cartGrid = expand.grid(.cp=(1:50)*0.01)
cv = train(Sentiment~. , data = trainsparse, method="rpart",
trControl = fitControl, tuneGrid = cartGrid)
cv
#create CART-Tree
tweetCART = rpart(Sentiment ~ ., data = trainsparse, method ="class", control = rpart.control(cp=0.01))
#plot Cart-Tree
prp(tweetCART)
#prediction from our model applied to our test-set
predictCart = predict(tweetCART, newdata = testsparse, type="class")
#Confusion Matrix
print(confusionMatrix(testsparse$Negative, predictCart))
#Confusion Matrix
print(confusionMatrix(testsparse$Sentiment, predictCart))
#Confusion Matrix
print(table(testsparse$Sentiment, predictCart))
library(ROCR)
#create ROC predict
predictROC = predict(tweetCART,newdata = testsparse)
print(table(testsparse$Sentiment,predictTreeROC[,2]>0.5))
print(table(testsparse$Sentiment,predictROC[,2]>0.5))
ROCRpred = prediction(predictTreeROC[,2],testsparse$Sentiment)
ROCRpred = prediction(predictROC[,2],testsparse$Sentiment)
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
(40+16+26)/(53+42+55)
table(twts$Sentiment)
table(testsparse$Sentiment)
cv
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
cv
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
cv
cv
confusionMatrix(testsparse$Sentiment)
confusionMatrix(testsparse$Sentiment, predictCart)
table(testsparse$Sentiment)
53/150
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
plot(predictCart)
plot(testsparse$Sentiment, predictCart)
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
tinytex::install_tinytex()
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
testsparse$Negative
table(testsparse$Negative)
53/149
96/149
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
table(testsparse$Sentiment)
53/150
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
testsparse$Negative
table((testsparse$Negative)
)
53/149
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
table((testsparse$Negative)>0.5)
18/35+18
18/(35+18)
table((testsparse$Negative)
)
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/sentimentCart.R')
table((testsparse$Negative))
53/149
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
source('D:/GitHub/projects/analytics/AL_opd4/AL_opd4/3Sentiment.R')
table(testsparse$Sentiment)
53/(150)
